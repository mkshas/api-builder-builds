name: Build JAR

on:
  workflow_dispatch:
    inputs:
      jobId:
        description: 'Build job ID'
        required: true
        type: string
      definitionId:
        description: 'API Definition ID'
        required: true
        type: string
      usePreviewBucket:
        description: 'Use preview R2 bucket (true when API Builder runs locally)'
        required: false
        type: boolean
        default: false
      callbackUrl:
        description: 'Cloudflare callback URL (optional)'
        required: false
        type: string
      callbackToken:
        description: 'Callback authentication token (optional)'
        required: false
        type: string

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up JDK 11
        uses: actions/setup-java@v4
        with:
          java-version: '11'
          distribution: 'temurin'

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install R2 script dependencies
        run: npm install @aws-sdk/client-s3

      - name: Set R2 bucket (preview vs production)
        run: |
          if [ -n "$R2_BUCKET_BASE" ]; then
            SUFFIX=""
            [ "$USE_PREVIEW_BUCKET" = "true" ] && SUFFIX="-preview"
            BUCKET="${R2_BUCKET_BASE}${SUFFIX}"
          else
            BUCKET="$R2_BUCKET_NAME_FALLBACK"
          fi
          echo "R2_BUCKET_NAME=$BUCKET" >> $GITHUB_ENV
          echo "Using bucket: $BUCKET"
        env:
          R2_BUCKET_BASE: ${{ secrets.R2_BUCKET_BASE }}
          R2_BUCKET_NAME_FALLBACK: ${{ secrets.R2_BUCKET_NAME }}
          USE_PREVIEW_BUCKET: ${{ inputs.usePreviewBucket }}

      - name: Check package exists in R2
        run: |
          node -e "
          const { S3Client, HeadObjectCommand } = require('@aws-sdk/client-s3');
          const client = new S3Client({
            region: 'auto',
            endpoint: process.env.R2_ENDPOINT,
            credentials: {
              accessKeyId: process.env.R2_ACCESS_KEY_ID,
              secretAccessKey: process.env.R2_SECRET_ACCESS_KEY
            }
          });
          const key = 'packages/${{ inputs.definitionId }}/field-mapping.json';
          (async () => {
            try {
              await client.send(new HeadObjectCommand({ Bucket: process.env.R2_BUCKET_NAME, Key: key }));
              console.log('Package found in R2');
            } catch (e) {
              console.error('Package not found. Run package generation first for this definition.');
              console.error('Expected: ' + key);
              console.error('Bucket: ' + process.env.R2_BUCKET_NAME);
              console.error('Hint: If you used the local API Builder, R2_BUCKET_NAME must be api-bldr-data-cache-r2-preview. For production: api-bldr-data-cache-r2.');
              process.exit(1);
            }
          })();
          "
        env:
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}

      - name: Download package files from R2
        id: download
        run: |
          node -e "
          const { S3Client, GetObjectCommand } = require('@aws-sdk/client-s3');
          const fs = require('fs');
          const path = require('path');

          const client = new S3Client({
            region: 'auto',
            endpoint: process.env.R2_ENDPOINT,
            credentials: {
              accessKeyId: process.env.R2_ACCESS_KEY_ID,
              secretAccessKey: process.env.R2_SECRET_ACCESS_KEY
            }
          });

          const bucket = process.env.R2_BUCKET_NAME;
          const definitionId = '${{ inputs.definitionId }}';
          const prefix = 'packages/' + definitionId + '/';

          async function get(key) {
            try {
              const cmd = new GetObjectCommand({ Bucket: bucket, Key: key });
              const res = await client.send(cmd);
              return await res.Body.transformToString();
            } catch (e) {
              const msg = e.message || '';
              const hint = msg.includes('does not exist') || msg.includes('NoSuchKey')
                ? ' Run package generation first, or ensure R2_BUCKET_NAME matches where the API Builder uploaded (api-bldr-data-cache-r2-preview for local, api-bldr-data-cache-r2 for prod).'
                : '';
              throw new Error('Failed to get ' + key + ': ' + msg + hint);
            }
          }

          (async () => {
            // 1. field-mapping.json
            const fm = JSON.parse(await get(prefix + 'field-mapping.json'));
            const apiPack = (fm.apiPack || 'api').toLowerCase();
            const version = fm.version || '1.0';
            const handlers = fm.handlers || [];

            fs.writeFileSync('/tmp/field-mapping.json', JSON.stringify(fm));
            fs.writeFileSync('/tmp/api-pack.txt', apiPack);
            fs.writeFileSync('/tmp/version.txt', version);
            fs.writeFileSync('/tmp/handlers.json', JSON.stringify(handlers));

            // 2. servlet
            const servlet = await get(prefix + 'servlet.java');
            const servletPath = 'src/main/java/com/tririga/custom/' + apiPack + '.java';
            fs.mkdirSync(path.dirname(servletPath), { recursive: true });
            fs.writeFileSync(servletPath, servlet);

            // 3. Remove template placeholder
            try { fs.unlinkSync('src/main/java/com/tririga/custom/{{API_PACK_NAME}}.java'); } catch (_) {}

            // 4. FieldMapper
            const fieldMapper = await get(prefix + 'FieldMapper.java');
            fs.mkdirSync('src/main/java/com/konvergex/apigen/common', { recursive: true });
            fs.writeFileSync('src/main/java/com/konvergex/apigen/common/FieldMapper.java', fieldMapper);

            // 5. Handlers
            fs.mkdirSync('src/main/java/com/konvergex/apigen/handlers', { recursive: true });
            for (const h of handlers) {
              const content = await get(prefix + 'handlers/' + h + '.java');
              fs.writeFileSync('src/main/java/com/konvergex/apigen/handlers/' + h + '.java', content);
            }

            // 6. openapi.json - for /doc and /openapi.json endpoints (servlet reads from classpath)
            const openapi = await get(prefix + 'openapi.json');
            fs.mkdirSync('src/main/resources', { recursive: true });
            fs.writeFileSync('src/main/resources/openapi.json', openapi);

            console.log('Downloaded package files');
          })();
          "
        env:
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}

      - name: Replace placeholders in build files
        run: |
          API_PACK=$(cat /tmp/api-pack.txt)
          VERSION=$(cat /tmp/version.txt)
          sed -i "s/{{API_PACK_NAME}}/$API_PACK/g" build.gradle
          sed -i "s/{{VERSION}}/$VERSION/g" build.gradle
          sed -i "s/{{API_PACK_NAME}}/$API_PACK/g" settings.gradle

      - name: Build with Gradle
        run: ./gradlew build --no-daemon --stacktrace

      - name: Find generated JAR
        id: find-jar
        run: |
          JAR_FILE=$(find build/libs -name "*.jar" ! -name "*-sources.jar" ! -name "*-javadoc.jar" | head -1)
          echo "jar_path=$JAR_FILE" >> $GITHUB_OUTPUT
          echo "jar_name=$(basename $JAR_FILE)" >> $GITHUB_OUTPUT

      - name: Upload JAR to R2
        run: |
          node -e "
          const { S3Client, PutObjectCommand } = require('@aws-sdk/client-s3');
          const fs = require('fs');
          const client = new S3Client({
            region: 'auto',
            endpoint: process.env.R2_ENDPOINT,
            credentials: {
              accessKeyId: process.env.R2_ACCESS_KEY_ID,
              secretAccessKey: process.env.R2_SECRET_ACCESS_KEY
            }
          });
          const jarPath = '${{ steps.find-jar.outputs.jar_path }}';
          const jarData = fs.readFileSync(jarPath);
          const command = new PutObjectCommand({
            Bucket: process.env.R2_BUCKET_NAME,
            Key: 'jars/${{ inputs.definitionId }}/${{ github.run_id }}_${{ steps.find-jar.outputs.jar_name }}',
            Body: jarData,
            ContentType: 'application/java-archive'
          });
          client.send(command).then(() => console.log('JAR uploaded')).catch(e => { console.error(e); process.exit(1); });
          "
        env:
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}

      - name: Upload build logs
        if: always()
        run: |
          mkdir -p /tmp/logs
          echo "${{ github.run_id }}" > /tmp/logs/run_id.txt
          echo "${{ github.run_number }}" > /tmp/logs/run_number.txt
          echo "${{ github.workflow }}" > /tmp/logs/workflow.txt
          echo "${{ github.repository }}" > /tmp/logs/repository.txt

          node -e "
          const { S3Client, PutObjectCommand } = require('@aws-sdk/client-s3');
          const fs = require('fs');
          const client = new S3Client({
            region: 'auto',
            endpoint: process.env.R2_ENDPOINT,
            credentials: {
              accessKeyId: process.env.R2_ACCESS_KEY_ID,
              secretAccessKey: process.env.R2_SECRET_ACCESS_KEY
            }
          });
          const files = ['run_id.txt', 'run_number.txt', 'workflow.txt', 'repository.txt'];
          Promise.all(files.map(filename => {
            const filePath = '/tmp/logs/' + filename;
            if (fs.existsSync(filePath)) {
              const data = fs.readFileSync(filePath);
              return client.send(new PutObjectCommand({
                Bucket: process.env.R2_BUCKET_NAME,
                Key: 'build-logs/${{ inputs.jobId }}/' + filename,
                Body: data
              }));
            }
          })).then(() => console.log('Logs uploaded'));
          "
        env:
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
          R2_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}

      - name: Notify Cloudflare (if callback provided)
        if: always() && inputs.callbackUrl != ''
        run: |
          STATUS="${{ job.status }}"
          if [ "$STATUS" = "success" ]; then
            STATUS="completed"
          else
            STATUS="failed"
          fi

          JAR_NAME="${{ steps.find-jar.outputs.jar_name }}"
          [ -z "$JAR_NAME" ] && JAR_NAME="unknown.jar"

          curl -X POST "${{ inputs.callbackUrl }}" \
            -H "Authorization: Bearer ${{ inputs.callbackToken }}" \
            -H "Content-Type: application/json" \
            -d "{
              \"jobId\": \"${{ inputs.jobId }}\",
              \"status\": \"$STATUS\",
              \"runId\": \"${{ github.run_id }}\",
              \"runNumber\": \"${{ github.run_number }}\",
              \"jarR2Key\": \"jars/${{ inputs.definitionId }}/${{ github.run_id }}_$JAR_NAME\",
              \"workflowUrl\": \"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"
            }"
