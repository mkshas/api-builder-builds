name: Build JAR

on:
  workflow_dispatch:
    inputs:
      jobId:
        description: 'Build job ID'
        required: true
        type: string
      definitionId:
        description: 'API Definition ID'
        required: true
        type: string
      usePreviewBucket:
        description: 'Use preview R2 bucket (true when API Builder runs locally)'
        required: false
        type: boolean
        default: false
      callbackUrl:
        description: 'Cloudflare callback URL (optional)'
        required: false
        type: string
      callbackToken:
        description: 'Callback authentication token (optional)'
        required: false
        type: string

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up JDK 11
        uses: actions/setup-java@v4
        with:
          java-version: '11'
          distribution: 'temurin'

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install R2 script dependencies
        run: npm install @aws-sdk/client-s3

      - name: Set R2 bucket (preview vs production)
        id: bucket
        run: |
          echo "Bucket sources: var=$([ -n \"$R2_BUCKET_NAME_VAR\" ] && echo 'set' || echo 'empty') base=$([ -n \"$R2_BUCKET_BASE\" ] && echo 'set' || echo 'empty') fallback=$([ -n \"$R2_BUCKET_NAME_FALLBACK\" ] && echo 'set' || echo 'empty') usePreviewBucket=$USE_PREVIEW_BUCKET"
          if [ -n "$R2_BUCKET_NAME_VAR" ]; then
            BUCKET="$R2_BUCKET_NAME_VAR"
            MODE="variable"
          elif [ -n "$R2_BUCKET_BASE" ]; then
            SUFFIX=""
            [ "$USE_PREVIEW_BUCKET" = "true" ] && SUFFIX="-preview"
            BUCKET="${R2_BUCKET_BASE}${SUFFIX}"
            MODE="$([ "$USE_PREVIEW_BUCKET" = "true" ] && echo 'preview' || echo 'production')"
          elif [ -n "$R2_BUCKET_NAME_FALLBACK" ]; then
            BUCKET="$R2_BUCKET_NAME_FALLBACK"
            MODE="secret"
          else
            echo "::error title=R2 bucket not configured::Set R2_BUCKET_NAME (variable) or R2_BUCKET_BASE (secret) or R2_BUCKET_NAME (secret). Repo: Settings > Secrets and variables > Actions > Variables."
            exit 1
          fi
          if [ "$USE_PREVIEW_BUCKET" = "true" ] && ! echo "$BUCKET" | grep -q -- '-preview$'; then
            BUCKET="${BUCKET}-preview"
            MODE="preview (suffix applied)"
            echo "Applied -preview suffix (usePreviewBucket=true)"
          fi
          echo "R2_BUCKET_NAME=$BUCKET" >> $GITHUB_ENV
          echo "bucket_mode=$MODE" >> $GITHUB_OUTPUT
          echo "definition_id=${{ inputs.definitionId }}" >> $GITHUB_OUTPUT
          echo "::notice title=R2 config::Bucket=$BUCKET, mode=$MODE, definitionId=${{ inputs.definitionId }}"
          echo "Bucket: $BUCKET (mode=$MODE)"
          if [ "$USE_PREVIEW_BUCKET" = "true" ] && [ -n "$R2_ACCESS_KEY_ID_PREVIEW" ]; then
            echo "R2_ACCESS_KEY_ID=$R2_ACCESS_KEY_ID_PREVIEW" >> $GITHUB_ENV
            echo "R2_SECRET_ACCESS_KEY=$R2_SECRET_ACCESS_KEY_PREVIEW" >> $GITHUB_ENV
            echo "Using R2 credentials: preview (dev token)"
          else
            echo "R2_ACCESS_KEY_ID=$R2_ACCESS_KEY_ID_PROD" >> $GITHUB_ENV
            echo "R2_SECRET_ACCESS_KEY=$R2_SECRET_ACCESS_KEY_PROD" >> $GITHUB_ENV
            echo "Using R2 credentials: production (or fallback when preview not set)"
          fi
        env:
          R2_BUCKET_BASE: ${{ secrets.R2_BUCKET_BASE }}
          R2_BUCKET_NAME_VAR: ${{ vars.R2_BUCKET_NAME }}
          R2_BUCKET_NAME_FALLBACK: ${{ secrets.R2_BUCKET_NAME }}
          USE_PREVIEW_BUCKET: ${{ inputs.usePreviewBucket }}
          R2_ACCESS_KEY_ID_PREVIEW: ${{ secrets.R2_ACCESS_KEY_ID_PREVIEW }}
          R2_SECRET_ACCESS_KEY_PREVIEW: ${{ secrets.R2_SECRET_ACCESS_KEY_PREVIEW }}
          R2_ACCESS_KEY_ID_PROD: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY_PROD: ${{ secrets.R2_SECRET_ACCESS_KEY }}

      - name: List and verify package in R2
        run: |
          node -e "
          const { S3Client, ListObjectsV2Command, HeadObjectCommand, GetObjectCommand } = require('@aws-sdk/client-s3');

          const client = new S3Client({
            region: 'auto',
            endpoint: process.env.R2_ENDPOINT,
            credentials: {
              accessKeyId: process.env.R2_ACCESS_KEY_ID,
              secretAccessKey: process.env.R2_SECRET_ACCESS_KEY
            }
          });

          const bucket = process.env.R2_BUCKET_NAME;
          const endpoint = process.env.R2_ENDPOINT || '';
          const definitionId = '${{ inputs.definitionId }}';
          const prefix = 'packages/' + definitionId + '/';

          function buildUrl(operation, key) {
            const base = endpoint.endsWith('/') ? endpoint.slice(0, -1) : endpoint;
            if (operation === 'ListObjectsV2') {
              return base + '/' + bucket + '?list-type=2&prefix=' + encodeURIComponent(prefix);
            }
            let k = key || prefix;
            while (k.startsWith('/')) k = k.slice(1);
            return base + '/' + bucket + '/' + k;
          }

          console.log('=== R2 diagnostics ===');
          console.log('Endpoint:', endpoint);
          console.log('Bucket:', bucket || 'NOT SET');
          console.log('Prefix:', prefix);
          console.log('definitionId:', definitionId);
          console.log('');

          if (!bucket || bucket.trim() === '') {
            console.error('ERROR: R2_BUCKET_NAME is empty. Set it in repo Settings > Secrets and variables > Actions > Variables.');
            console.error('Variable name must be: R2_BUCKET_NAME');
            console.error('Value: api-bldr-data-cache-r2 (prod) or api-bldr-data-cache-r2-preview (local)');
            process.exit(1);
          }

          (async () => {
            // 1. List all objects under prefix
            let listed = [];
            const listUrl = buildUrl('ListObjectsV2');
            console.log('API: ListObjectsV2');
            console.log('URL:', listUrl);
            try {
              const list = await client.send(new ListObjectsV2Command({ Bucket: bucket, Prefix: prefix, MaxKeys: 100 }));
              listed = (list.Contents || []).map(o => o.Key).filter(Boolean);
              console.log('Objects found under ' + prefix + ':');
              if (listed.length === 0) {
                console.log('  (none)');
                console.error('');
                console.error('ERROR: No objects found. Package may not exist or wrong bucket.');
                console.error('API: ListObjectsV2 succeeded but returned 0 objects.');
                console.error('Fix: Run package generation in API Builder first, or check R2_BUCKET_BASE/R2_BUCKET_NAME matches where API Builder uploaded.');
                process.exit(1);
              }
              listed.forEach(k => console.log('  - ' + k));
              console.log('');
            } catch (e) {
              console.error('API: ListObjectsV2 FAILED');
              console.error('Error:', e.name, e.message);
              console.error('Code:', e.\$metadata?.httpStatusCode || e.Code || 'unknown');
              if (e.Code) console.error('AWS Code:', e.Code);
              throw e;
            }

            // 2. Must have field-mapping.json first (needed to know handlers)
            const requiredBase = ['field-mapping.json', 'servlet.java', 'FieldMapper.java', 'openapi.json'];
            const baseKey = prefix + 'field-mapping.json';

            console.log('');
            console.log('API: HeadObject field-mapping.json');
            console.log('URL:', buildUrl('GetObject', baseKey));
            let headErr = null;
            try {
              await client.send(new HeadObjectCommand({ Bucket: bucket, Key: baseKey }));
            } catch (e) {
              headErr = e;
            }
            if (headErr) {
              console.error('');
              console.error('ERROR: field-mapping.json not found or not accessible.');
              console.error('Key:', baseKey);
              console.error('API: HeadObject FAILED');
              console.error('Error:', headErr.name, headErr.message);
              console.error('Code:', headErr.\$metadata?.httpStatusCode || headErr.Code);
              process.exit(1);
            }

            // 3. Fetch field-mapping to get handlers, then verify all required files exist
            console.log('');
            console.log('API: GetObject field-mapping.json');
            console.log('URL:', buildUrl('GetObject', baseKey));
            const fmRes = await client.send(new GetObjectCommand({ Bucket: bucket, Key: baseKey }));
            const fm = JSON.parse(await fmRes.Body.transformToString());
            const handlers = fm.handlers || [];
            const requiredFiles = [...requiredBase, ...handlers.map(h => 'handlers/' + h + '.java')];

            const missing = [];
            for (const f of requiredFiles) {
              const key = prefix + f;
              try {
                await client.send(new HeadObjectCommand({ Bucket: bucket, Key: key }));
              } catch (e) {
                missing.push(f);
              }
            }

            if (missing.length > 0) {
              console.error('');
              console.error('ERROR: Required files missing in R2:');
              missing.forEach(f => console.error('  - ' + f));
              console.error('');
              console.error('Files present:', listed.join(', '));
              process.exit(1);
            }

            console.log('All required files verified:', requiredFiles.join(', '));
          })();
          "
        env:
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}

      - name: Download package files from R2
        id: download
        run: |
          node -e "
          const { S3Client, GetObjectCommand } = require('@aws-sdk/client-s3');
          const fs = require('fs');
          const path = require('path');

          const endpoint = process.env.R2_ENDPOINT || '';
          const bucket = process.env.R2_BUCKET_NAME;

          const client = new S3Client({
            region: 'auto',
            endpoint: endpoint,
            credentials: {
              accessKeyId: process.env.R2_ACCESS_KEY_ID,
              secretAccessKey: process.env.R2_SECRET_ACCESS_KEY
            }
          });

          function buildUrl(key) {
            const base = endpoint.endsWith('/') ? endpoint.slice(0, -1) : endpoint;
            let k = key;
            while (k.startsWith('/')) k = k.slice(1);
            return base + '/' + bucket + '/' + k;
          }

          console.log('Bucket:', bucket);
          console.log('Endpoint:', endpoint);
          const definitionId = '${{ inputs.definitionId }}';
          const prefix = 'packages/' + definitionId + '/';

          async function get(key, step) {
            const url = buildUrl(key);
            console.log('GetObject', step + ':', url);
            try {
              const cmd = new GetObjectCommand({ Bucket: bucket, Key: key });
              const res = await client.send(cmd);
              return await res.Body.transformToString();
            } catch (e) {
              console.error('');
              console.error('=== Download FAILED ===');
              console.error('File:', key);
              console.error('URL:', url);
              console.error('Step:', step);
              console.error('API: GetObject');
              console.error('Error:', e.name, e.message);
              console.error('Code:', e.\$metadata?.httpStatusCode || e.Code);
              if (e.Code) console.error('AWS Code:', e.Code);
              console.error('Bucket:', bucket || 'NOT SET');
              console.error('');
              throw e;
            }
          }

          (async () => {
            // 1. field-mapping.json
            const fm = JSON.parse(await get(prefix + 'field-mapping.json', '1/6'));
            const apiPack = (fm.apiPack || 'api').toLowerCase();
            const version = fm.version || '1.0';
            const handlers = fm.handlers || [];

            fs.writeFileSync('/tmp/field-mapping.json', JSON.stringify(fm));
            fs.writeFileSync('/tmp/api-pack.txt', apiPack);
            fs.writeFileSync('/tmp/version.txt', version);
            fs.writeFileSync('/tmp/handlers.json', JSON.stringify(handlers));

            // 2. servlet
            const servlet = await get(prefix + 'servlet.java', '2/6');
            const servletPath = 'src/main/java/com/tririga/custom/' + apiPack + '.java';
            fs.mkdirSync(path.dirname(servletPath), { recursive: true });
            fs.writeFileSync(servletPath, servlet);

            // 3. Remove template placeholder
            try { fs.unlinkSync('src/main/java/com/tririga/custom/{{API_PACK_NAME}}.java'); } catch (_) {}

            // 4. FieldMapper
            const fieldMapper = await get(prefix + 'FieldMapper.java', '4/6');
            fs.mkdirSync('src/main/java/com/konvergex/apigen/common', { recursive: true });
            fs.writeFileSync('src/main/java/com/konvergex/apigen/common/FieldMapper.java', fieldMapper);

            // 5. Handlers
            fs.mkdirSync('src/main/java/com/konvergex/apigen/handlers', { recursive: true });
            for (const h of handlers) {
              const content = await get(prefix + 'handlers/' + h + '.java', '5/6 handlers');
              fs.writeFileSync('src/main/java/com/konvergex/apigen/handlers/' + h + '.java', content);
            }

            // 6. openapi.json - for /doc and /openapi.json endpoints (servlet reads from classpath)
            const openapi = await get(prefix + 'openapi.json', '6/6');
            fs.mkdirSync('src/main/resources', { recursive: true });
            fs.writeFileSync('src/main/resources/openapi.json', openapi);

            console.log('Downloaded package files');
          })();
          "
        env:
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
          # R2_ACCESS_KEY_ID and R2_SECRET_ACCESS_KEY inherit from GITHUB_ENV (set by "Set R2 bucket" step)
          # which uses preview or production credentials based on usePreviewBucket

      - name: Replace placeholders in build files
        run: |
          API_PACK=$(cat /tmp/api-pack.txt)
          VERSION=$(cat /tmp/version.txt)
          sed -i "s/{{API_PACK_NAME}}/$API_PACK/g" build.gradle
          sed -i "s/{{VERSION}}/$VERSION/g" build.gradle
          sed -i "s/{{API_PACK_NAME}}/$API_PACK/g" settings.gradle

      - name: Build with Gradle
        run: ./gradlew build --no-daemon --stacktrace

      - name: Find generated JAR
        id: find-jar
        run: |
          JAR_FILE=$(find build/libs -name "*.jar" ! -name "*-sources.jar" ! -name "*-javadoc.jar" | head -1)
          echo "jar_path=$JAR_FILE" >> $GITHUB_OUTPUT
          echo "jar_name=$(basename $JAR_FILE)" >> $GITHUB_OUTPUT

      - name: Upload JAR to R2
        run: |
          node -e "
          const { S3Client, PutObjectCommand } = require('@aws-sdk/client-s3');
          const fs = require('fs');
          const client = new S3Client({
            region: 'auto',
            endpoint: process.env.R2_ENDPOINT,
            credentials: {
              accessKeyId: process.env.R2_ACCESS_KEY_ID,
              secretAccessKey: process.env.R2_SECRET_ACCESS_KEY
            }
          });
          const jarPath = '${{ steps.find-jar.outputs.jar_path }}';
          const jarData = fs.readFileSync(jarPath);
          const command = new PutObjectCommand({
            Bucket: process.env.R2_BUCKET_NAME,
            Key: 'jars/${{ inputs.definitionId }}/${{ github.run_id }}_${{ steps.find-jar.outputs.jar_name }}',
            Body: jarData,
            ContentType: 'application/java-archive'
          });
          client.send(command).then(() => console.log('JAR uploaded')).catch(e => { console.error(e); process.exit(1); });
          "
        env:
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}

      - name: Upload build logs
        if: always()
        run: |
          mkdir -p /tmp/logs
          echo "${{ github.run_id }}" > /tmp/logs/run_id.txt
          echo "${{ github.run_number }}" > /tmp/logs/run_number.txt
          echo "${{ github.workflow }}" > /tmp/logs/workflow.txt
          echo "${{ github.repository }}" > /tmp/logs/repository.txt

          node -e "
          const { S3Client, PutObjectCommand } = require('@aws-sdk/client-s3');
          const fs = require('fs');
          const client = new S3Client({
            region: 'auto',
            endpoint: process.env.R2_ENDPOINT,
            credentials: {
              accessKeyId: process.env.R2_ACCESS_KEY_ID,
              secretAccessKey: process.env.R2_SECRET_ACCESS_KEY
            }
          });
          const files = ['run_id.txt', 'run_number.txt', 'workflow.txt', 'repository.txt'];
          Promise.all(files.map(filename => {
            const filePath = '/tmp/logs/' + filename;
            if (fs.existsSync(filePath)) {
              const data = fs.readFileSync(filePath);
              return client.send(new PutObjectCommand({
                Bucket: process.env.R2_BUCKET_NAME,
                Key: 'build-logs/${{ inputs.jobId }}/' + filename,
                Body: data
              }));
            }
          })).then(() => console.log('Logs uploaded'));
          "
        env:
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}

      - name: Notify Cloudflare (if callback provided)
        if: always() && inputs.callbackUrl != ''
        run: |
          STATUS="${{ job.status }}"
          if [ "$STATUS" = "success" ]; then
            STATUS="completed"
          else
            STATUS="failed"
          fi

          JAR_NAME="${{ steps.find-jar.outputs.jar_name }}"
          [ -z "$JAR_NAME" ] && JAR_NAME="unknown.jar"

          curl -X POST "${{ inputs.callbackUrl }}" \
            -H "Authorization: Bearer ${{ inputs.callbackToken }}" \
            -H "Content-Type: application/json" \
            -d "{
              \"jobId\": \"${{ inputs.jobId }}\",
              \"status\": \"$STATUS\",
              \"runId\": \"${{ github.run_id }}\",
              \"runNumber\": \"${{ github.run_number }}\",
              \"jarR2Key\": \"jars/${{ inputs.definitionId }}/${{ github.run_id }}_$JAR_NAME\",
              \"workflowUrl\": \"${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"
            }"
