name: Build JAR

on:
  workflow_dispatch:
    inputs:
      jobId:
        description: 'Build job ID'
        required: true
        type: string
      definitionId:
        description: 'API Definition ID'
        required: true
        type: string
      usePreviewBucket:
        description: 'Use preview R2 bucket (true when API Builder runs locally)'
        required: false
        type: boolean
        default: false
      callbackUrl:
        description: 'Cloudflare callback URL (optional)'
        required: false
        type: string
      callbackToken:
        description: 'Callback authentication token (optional)'
        required: false
        type: string

jobs:
  build:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up JDK 11
        uses: actions/setup-java@v4
        with:
          java-version: '11'
          distribution: 'temurin'

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install R2 script dependencies
        run: npm install @aws-sdk/client-s3

      - name: Set R2 bucket (preview vs production)
        id: bucket
        run: |
          echo "Bucket sources: var=$([ -n \"$R2_BUCKET_NAME_VAR\" ] && echo 'set' || echo 'empty') base=$([ -n \"$R2_BUCKET_BASE\" ] && echo 'set' || echo 'empty') fallback=$([ -n \"$R2_BUCKET_NAME_FALLBACK\" ] && echo 'set' || echo 'empty') usePreviewBucket=$USE_PREVIEW_BUCKET"
          if [ -n "$R2_BUCKET_NAME_VAR" ]; then
            BUCKET="$R2_BUCKET_NAME_VAR"
            MODE="variable"
          elif [ -n "$R2_BUCKET_BASE" ]; then
            SUFFIX=""
            [ "$USE_PREVIEW_BUCKET" = "true" ] && SUFFIX="-preview"
            BUCKET="${R2_BUCKET_BASE}${SUFFIX}"
            MODE="$([ "$USE_PREVIEW_BUCKET" = "true" ] && echo 'preview' || echo 'production')"
          elif [ -n "$R2_BUCKET_NAME_FALLBACK" ]; then
            BUCKET="$R2_BUCKET_NAME_FALLBACK"
            MODE="secret"
          else
            echo "::error title=R2 bucket not configured::Set R2_BUCKET_NAME (variable) or R2_BUCKET_BASE (secret) or R2_BUCKET_NAME (secret). Repo: Settings > Secrets and variables > Actions > Variables."
            exit 1
          fi
          if [ "$USE_PREVIEW_BUCKET" = "true" ] && ! echo "$BUCKET" | grep -q -- '-preview$'; then
            BUCKET="${BUCKET}-preview"
            MODE="preview (suffix applied)"
            echo "Applied -preview suffix (usePreviewBucket=true)"
          fi
          echo "R2_BUCKET_NAME=$BUCKET" >> $GITHUB_ENV
          echo "bucket_mode=$MODE" >> $GITHUB_OUTPUT
          echo "definition_id=${{ inputs.definitionId }}" >> $GITHUB_OUTPUT
          echo "::notice title=R2 config::Bucket=$BUCKET, mode=$MODE, definitionId=${{ inputs.definitionId }}"
          echo "Bucket: $BUCKET (mode=$MODE)"
          if [ "$USE_PREVIEW_BUCKET" = "true" ] && [ -n "$R2_ACCESS_KEY_ID_PREVIEW" ]; then
            echo "R2_ACCESS_KEY_ID=$R2_ACCESS_KEY_ID_PREVIEW" >> $GITHUB_ENV
            echo "R2_SECRET_ACCESS_KEY=$R2_SECRET_ACCESS_KEY_PREVIEW" >> $GITHUB_ENV
            echo "Using R2 credentials: preview (dev token)"
          else
            echo "R2_ACCESS_KEY_ID=$R2_ACCESS_KEY_ID_PROD" >> $GITHUB_ENV
            echo "R2_SECRET_ACCESS_KEY=$R2_SECRET_ACCESS_KEY_PROD" >> $GITHUB_ENV
            echo "Using R2 credentials: production (or fallback when preview not set)"
          fi
        env:
          R2_BUCKET_BASE: ${{ secrets.R2_BUCKET_BASE }}
          R2_BUCKET_NAME_VAR: ${{ vars.R2_BUCKET_NAME }}
          R2_BUCKET_NAME_FALLBACK: ${{ secrets.R2_BUCKET_NAME }}
          USE_PREVIEW_BUCKET: ${{ inputs.usePreviewBucket }}
          R2_ACCESS_KEY_ID_PREVIEW: ${{ secrets.R2_ACCESS_KEY_ID_PREVIEW }}
          R2_SECRET_ACCESS_KEY_PREVIEW: ${{ secrets.R2_SECRET_ACCESS_KEY_PREVIEW }}
          R2_ACCESS_KEY_ID_PROD: ${{ secrets.R2_ACCESS_KEY_ID }}
          R2_SECRET_ACCESS_KEY_PROD: ${{ secrets.R2_SECRET_ACCESS_KEY }}

      - name: List and verify package in R2
        run: |
          node -e "
          const { S3Client, ListObjectsV2Command, HeadObjectCommand, GetObjectCommand } = require('@aws-sdk/client-s3');

          const client = new S3Client({
            region: 'auto',
            endpoint: process.env.R2_ENDPOINT,
            credentials: {
              accessKeyId: process.env.R2_ACCESS_KEY_ID,
              secretAccessKey: process.env.R2_SECRET_ACCESS_KEY
            }
          });

          const bucket = process.env.R2_BUCKET_NAME;
          const endpoint = process.env.R2_ENDPOINT || '';
          const definitionId = '${{ inputs.definitionId }}';
          const prefix = 'packages/' + definitionId + '/';

          function buildUrl(operation, key) {
            const base = endpoint.endsWith('/') ? endpoint.slice(0, -1) : endpoint;
            if (operation === 'ListObjectsV2') {
              return base + '/' + bucket + '?list-type=2&prefix=' + encodeURIComponent(prefix);
            }
            let k = key || prefix;
            while (k.startsWith('/')) k = k.slice(1);
            return base + '/' + bucket + '/' + k;
          }

          console.log('=== R2 diagnostics ===');
          console.log('Endpoint:', endpoint);
          console.log('Bucket:', bucket || 'NOT SET');
          console.log('Prefix:', prefix);
          console.log('definitionId:', definitionId);
          console.log('');

          if (!bucket || bucket.trim() === '') {
            console.error('ERROR: R2_BUCKET_NAME is empty. Set it in repo Settings > Secrets and variables > Actions > Variables.');
            console.error('Variable name must be: R2_BUCKET_NAME');
            console.error('Value: api-bldr-data-cache-r2 (prod) or api-bldr-data-cache-r2-preview (local)');
            process.exit(1);
          }

          (async () => {
            // 1. List all objects under prefix
            let listed = [];
            const listUrl = buildUrl('ListObjectsV2');
            console.log('API: ListObjectsV2');
            console.log('URL:', listUrl);
            try {
              const list = await client.send(new ListObjectsV2Command({ Bucket: bucket, Prefix: prefix, MaxKeys: 100 }));
              listed = (list.Contents || []).map(o => o.Key).filter(Boolean);
              console.log('Objects found under ' + prefix + ':');
              if (listed.length === 0) {
                console.log('  (none)');
                console.error('');
                console.error('ERROR: No objects found. Package may not exist or wrong bucket.');
                console.error('API: ListObjectsV2 succeeded but returned 0 objects.');
                console.error('Fix: Run package generation in API Builder first, or check R2_BUCKET_BASE/R2_BUCKET_NAME matches where API Builder uploaded.');
                process.exit(1);
              }
              listed.forEach(k => console.log('  - ' + k));
              console.log('');
            } catch (e) {
              console.error('API: ListObjectsV2 FAILED');
              console.error('Error:', e.name, e.message);
              console.error('Code:', e.\$metadata?.httpStatusCode || e.Code || 'unknown');
              if (e.Code) console.error('AWS Code:', e.Code);
              throw e;
            }

            // 2. Must have field-mapping.json first (needed to know handlers)
            const requiredBase = ['field-mapping.json', 'servlet.java', 'FieldMapper.java', 'openapi.json'];
            const baseKey = prefix + 'field-mapping.json';

            console.log('');
            console.log('API: HeadObject field-mapping.json');
            console.log('URL:', buildUrl('GetObject', baseKey));
            let headErr = null;
            try {
              await client.send(new HeadObjectCommand({ Bucket: bucket, Key: baseKey }));
            } catch (e) {
              headErr = e;
            }
            if (headErr) {
              console.error('');
              console.error('ERROR: field-mapping.json not found or not accessible.');
              console.error('Key:', baseKey);
              console.error('API: HeadObject FAILED');
              console.error('Error:', headErr.name, headErr.message);
              console.error('Code:', headErr.\$metadata?.httpStatusCode || headErr.Code);
              process.exit(1);
            }

            // 3. Fetch field-mapping to get handlers, then verify all required files exist
            console.log('');
            console.log('API: GetObject field-mapping.json');
            console.log('URL:', buildUrl('GetObject', baseKey));
            const fmRes = await client.send(new GetObjectCommand({ Bucket: bucket, Key: baseKey }));
            const fm = JSON.parse(await fmRes.Body.transformToString());
            const handlers = fm.handlers || [];
            const reports = fm.reports || [];
            console.log('field-mapping.reports:', JSON.stringify(reports));
            if (reports.length === 0) {
              console.warn('WARN: field-mapping has no "reports" array. Report classes (e.g. Report_triWorkTask) will NOT be in JAR. Re-run Create API to regenerate package with Report classes.');
            }
            const requiredFiles = [...requiredBase, ...handlers.map(h => 'handlers/' + h + '.java')];
            const optionalReportFiles = reports.map(r => 'reports/' + r + '.java');

            const missing = [];
            for (const f of requiredFiles) {
              const key = prefix + f;
              try {
                await client.send(new HeadObjectCommand({ Bucket: bucket, Key: key }));
              } catch (e) {
                missing.push(f);
              }
            }

            if (missing.length > 0) {
              console.error('');
              console.error('ERROR: Required files missing in R2:');
              missing.forEach(f => console.error('  - ' + f));
              console.error('');
              console.error('Files present:', listed.join(', '));
              process.exit(1);
            }

            console.log('All required files verified:', requiredFiles.join(', '));
            if (optionalReportFiles.length > 0) {
              console.log('Optional Report classes:', optionalReportFiles.join(', '));
            }
          })();
          "
        env:
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}

      - name: Download package files from R2
        id: download
        run: |
          node -e "
          const { S3Client, GetObjectCommand } = require('@aws-sdk/client-s3');
          const fs = require('fs');
          const path = require('path');

          const endpoint = process.env.R2_ENDPOINT || '';
          const bucket = process.env.R2_BUCKET_NAME;

          const client = new S3Client({
            region: 'auto',
            endpoint: endpoint,
            credentials: {
              accessKeyId: process.env.R2_ACCESS_KEY_ID,
              secretAccessKey: process.env.R2_SECRET_ACCESS_KEY
            }
          });

          function buildUrl(key) {
            const base = endpoint.endsWith('/') ? endpoint.slice(0, -1) : endpoint;
            let k = key;
            while (k.startsWith('/')) k = k.slice(1);
            return base + '/' + bucket + '/' + k;
          }

          console.log('Bucket:', bucket);
          console.log('Endpoint:', endpoint);
          const definitionId = '${{ inputs.definitionId }}';
          const prefix = 'packages/' + definitionId + '/';

          async function get(key, step) {
            const url = buildUrl(key);
            console.log('GetObject', step + ':', url);
            try {
              const cmd = new GetObjectCommand({ Bucket: bucket, Key: key });
              const res = await client.send(cmd);
              return await res.Body.transformToString();
            } catch (e) {
              console.error('');
              console.error('=== Download FAILED ===');
              console.error('File:', key);
              console.error('URL:', url);
              console.error('Step:', step);
              console.error('API: GetObject');
              console.error('Error:', e.name, e.message);
              console.error('Code:', e.\$metadata?.httpStatusCode || e.Code);
              if (e.Code) console.error('AWS Code:', e.Code);
              console.error('Bucket:', bucket || 'NOT SET');
              console.error('');
              throw e;
            }
          }

          (async () => {
            // 1. field-mapping.json
            const fm = JSON.parse(await get(prefix + 'field-mapping.json', '1/7'));
            const apiPack = (fm.apiPack || 'api').toLowerCase();
            const version = fm.version || '1.0';
            const handlers = fm.handlers || [];

            fs.writeFileSync('/tmp/field-mapping.json', JSON.stringify(fm));
            fs.writeFileSync('/tmp/api-pack.txt', apiPack);
            fs.writeFileSync('/tmp/version.txt', version);
            fs.writeFileSync('/tmp/handlers.json', JSON.stringify(handlers));

            // 2. servlet
            const servlet = await get(prefix + 'servlet.java', '2/7');
            const servletPath = 'src/main/java/com/tririga/custom/' + apiPack + '.java';
            fs.mkdirSync(path.dirname(servletPath), { recursive: true });
            fs.writeFileSync(servletPath, servlet);

            // 3. Remove template placeholder
            try { fs.unlinkSync('src/main/java/com/tririga/custom/{{API_PACK_NAME}}.java'); } catch (_) {}

            // 4. FieldMapper
            const fieldMapper = await get(prefix + 'FieldMapper.java', '4/7');
            fs.mkdirSync('src/main/java/com/konvergex/apigen/common', { recursive: true });
            fs.writeFileSync('src/main/java/com/konvergex/apigen/common/FieldMapper.java', fieldMapper);

            // 5. Handlers
            fs.mkdirSync('src/main/java/com/konvergex/apigen/handlers', { recursive: true });
            for (const h of handlers) {
              const content = await get(prefix + 'handlers/' + h + '.java', '5/7 handlers');
              fs.writeFileSync('src/main/java/com/konvergex/apigen/handlers/' + h + '.java', content);
            }

            // 5b. Report classes (for GET list; BO.getRecords uses these)
            const reports = fm.reports || [];
            fs.mkdirSync('src/main/java/com/konvergex/apigen/reports', { recursive: true });
            for (const reportName of reports) {
              try {
                const content = await get(prefix + 'reports/' + reportName + '.java', '5b/7 reports');
                fs.writeFileSync('src/main/java/com/konvergex/apigen/reports/' + reportName + '.java', content);
                console.log('Downloaded report class:', reportName + '.java');
              } catch (e) {
                console.error('ERROR: Report class download failed for', reportName + '.java', '-', e.message);
                throw new Error('Report class ' + reportName + ' is in field-mapping but could not be downloaded from R2. Check bucket/prefix and that package was regenerated.');
              }
            }

            // 6. openapi.json - for /doc and /openapi.json endpoints (servlet reads from classpath)
            const openapi = await get(prefix + 'openapi.json', '6/7');
            fs.mkdirSync('src/main/resources', { recursive: true });
            fs.writeFileSync('src/main/resources/openapi.json', openapi);

            console.log('Downloaded package files');
          })();
          "
        env:
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}
          # R2_ACCESS_KEY_ID and R2_SECRET_ACCESS_KEY inherit from GITHUB_ENV (set by "Set R2 bucket" step)
          # which uses preview or production credentials based on usePreviewBucket

      - name: Replace placeholders in build files
        run: |
          API_PACK=$(cat /tmp/api-pack.txt)
          VERSION=$(cat /tmp/version.txt)
          sed -i "s/{{API_PACK_NAME}}/$API_PACK/g" build.gradle
          sed -i "s/{{VERSION}}/$VERSION/g" build.gradle
          sed -i "s/{{API_PACK_NAME}}/$API_PACK/g" settings.gradle

      - name: Build with Gradle
        run: ./gradlew build --no-daemon --stacktrace

      - name: Find generated JAR
        id: find-jar
        run: |
          JAR_FILE=$(find build/libs -name "*.jar" ! -name "*-sources.jar" ! -name "*-javadoc.jar" | head -1)
          echo "jar_path=$JAR_FILE" >> $GITHUB_OUTPUT
          echo "jar_name=$(basename $JAR_FILE)" >> $GITHUB_OUTPUT

      - name: Upload JAR and dependencies to R2
        id: upload-jars
        run: |
          node -e "
          (async () => {
            const { S3Client, PutObjectCommand } = require('@aws-sdk/client-s3');
            const fs = require('fs');
            const path = require('path');
            const { execSync } = require('child_process');

            const client = new S3Client({
              region: 'auto',
              endpoint: process.env.R2_ENDPOINT,
              credentials: {
                accessKeyId: process.env.R2_ACCESS_KEY_ID,
                secretAccessKey: process.env.R2_SECRET_ACCESS_KEY
              }
            });

            const definitionId = '${{ inputs.definitionId }}';
            const runId = '${{ github.run_id }}';
            const bucket = process.env.R2_BUCKET_NAME;
            const jarPath = '${{ steps.find-jar.outputs.jar_path }}';
            const jarName = '${{ steps.find-jar.outputs.jar_name }}';

            // Upload main JAR
            const jarData = fs.readFileSync(jarPath);
            const jarR2Key = 'jars/' + definitionId + '/' + runId + '_' + jarName;
            await client.send(new PutObjectCommand({
              Bucket: bucket,
              Key: jarR2Key,
              Body: jarData,
              ContentType: 'application/java-archive'
            }));
            console.log('Main JAR uploaded:', jarR2Key);

            // Find and upload dependency JARs (build/libs/dependencies/*.jar)
            const dependentJarR2Keys = [];
            let depJars = [];
            try {
              depJars = execSync('find build/libs/dependencies -name \"*.jar\" 2>/dev/null', { encoding: 'utf8' })
                .trim().split('\n').filter(Boolean);
            } catch (_) {}

            const depsPrefix = 'jars/' + definitionId + '/' + runId + '_deps/';
            for (const depPath of depJars) {
              const depName = path.basename(depPath);
              const depKey = depsPrefix + depName;
              const depData = fs.readFileSync(depPath);
              await client.send(new PutObjectCommand({
                Bucket: bucket,
                Key: depKey,
                Body: depData,
                ContentType: 'application/java-archive'
              }));
              dependentJarR2Keys.push(depKey);
              console.log('Dependency JAR uploaded:', depKey);
            }

            if (dependentJarR2Keys.length === 0) {
              console.log('No runtime dependencies to upload (expected: json-20231013.jar from lib/)');
            }

            fs.appendFileSync(process.env.GITHUB_OUTPUT,
              'jarR2Key=' + jarR2Key + '\n' +
              'dependentJarR2Keys=' + JSON.stringify(dependentJarR2Keys) + '\n'
            );
          })().catch(e => { console.error(e); process.exit(1); });
          "
        env:
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}

      - name: Upload build logs
        if: always()
        run: |
          mkdir -p /tmp/logs
          echo "${{ github.run_id }}" > /tmp/logs/run_id.txt
          echo "${{ github.run_number }}" > /tmp/logs/run_number.txt
          echo "${{ github.workflow }}" > /tmp/logs/workflow.txt
          echo "${{ github.repository }}" > /tmp/logs/repository.txt

          node -e "
          const { S3Client, PutObjectCommand } = require('@aws-sdk/client-s3');
          const fs = require('fs');
          const client = new S3Client({
            region: 'auto',
            endpoint: process.env.R2_ENDPOINT,
            credentials: {
              accessKeyId: process.env.R2_ACCESS_KEY_ID,
              secretAccessKey: process.env.R2_SECRET_ACCESS_KEY
            }
          });
          const files = ['run_id.txt', 'run_number.txt', 'workflow.txt', 'repository.txt'];
          Promise.all(files.map(filename => {
            const filePath = '/tmp/logs/' + filename;
            if (fs.existsSync(filePath)) {
              const data = fs.readFileSync(filePath);
              return client.send(new PutObjectCommand({
                Bucket: process.env.R2_BUCKET_NAME,
                Key: 'build-logs/${{ inputs.jobId }}/' + filename,
                Body: data
              }));
            }
          })).then(() => console.log('Logs uploaded'));
          "
        env:
          R2_ENDPOINT: ${{ secrets.R2_ENDPOINT }}

      - name: Notify Cloudflare (if callback provided)
        if: always() && inputs.callbackUrl != ''
        run: |
          node -e "
          const callbackUrl = process.env.CALLBACK_URL;
          const callbackToken = process.env.CALLBACK_TOKEN;
          const jobId = process.env.JOB_ID;
          const status = process.env.STATUS === 'success' ? 'completed' : 'failed';
          const runId = process.env.RUN_ID;
          const runNumber = process.env.RUN_NUMBER;
          const jarR2Key = process.env.JAR_R2_KEY || '';
          let dependentJarR2Keys = [];
          try {
            dependentJarR2Keys = JSON.parse(process.env.DEPENDENT_JAR_R2_KEYS || '[]');
          } catch (_) {}
          const workflowUrl = process.env.WORKFLOW_URL;
          const body = JSON.stringify({
            jobId,
            status,
            runId: parseInt(runId, 10),
            runNumber: parseInt(runNumber, 10),
            jarR2Key,
            dependentJarR2Keys,
            workflowUrl
          });
          fetch(callbackUrl, {
            method: 'POST',
            headers: {
              'Authorization': 'Bearer ' + (callbackToken || ''),
              'Content-Type': 'application/json'
            },
            body
          }).then(r => {
            if (!r.ok) throw new Error('Callback failed: ' + r.status + ' ' + r.statusText);
            return r.text();
          }).then(t => console.log('Callback OK:', t)).catch(e => {
            console.error('Callback error:', e.message);
            process.exit(1);
          });
          "
        env:
          CALLBACK_URL: ${{ inputs.callbackUrl }}
          CALLBACK_TOKEN: ${{ inputs.callbackToken }}
          JOB_ID: ${{ inputs.jobId }}
          STATUS: ${{ job.status }}
          RUN_ID: ${{ github.run_id }}
          RUN_NUMBER: ${{ github.run_number }}
          JAR_R2_KEY: ${{ steps.upload-jars.outputs.jarR2Key }}
          DEPENDENT_JAR_R2_KEYS: ${{ steps.upload-jars.outputs.dependentJarR2Keys }}
          WORKFLOW_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
